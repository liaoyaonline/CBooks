# Linux网络爬虫之避免重复爬取(1)
## 网络爬虫概述
&emsp;&emsp;我先说下我自己对网络爬虫的理解，就是抓取一个网页，将该网页中需要的数据和需要的超链接进行保存，其中需要的数据进行二次加工存入我们的数据库，需要的超链接进行加工存入待爬取队列。从待爬取队列中抓取一个url，重复上述步骤。爬虫就是从待爬取队列选择url抓取网页，提取超链接加入待爬取队列。不断重复，直到达到结束条件，就像一个永不止步的火车，我们要做的就是让火车上搭载我们的私活，比方说在每次爬取网页的时候将我们需要的数据保存到数据库。其运行简图如下图。

![](/home/liaoya/图片/2019-12-18 19-28-34 的屏幕截图.png) 

第二步和第六步，都是为了避免重复爬取，减少工作量。
## 避免重复爬取思路
&emsp;&emsp;怎么避免重复爬取，对于这种大的数据的校验，第一就是想到hashmap，它能够基本做到O(1)的速度查询。在这里对hashmap做一下简单介绍，个人理解是，hashmap就是将字符串通过hash函数后形成一个数字，将字符串对应信息存储在以该数字为下标的数组中。查询的时候能够直接通过定位到信息。使用hashmap要解决哈希冲突，哈希冲突即几个不同的字符串通过hash函数后形成同一个数字。解决哈希冲突的方法有：1，根据数据特点设计比较好的hash函数，让字符串通过hash函数后形成的值在数组中分布比较均匀。2，通过拉链法在出现哈希冲突时通过叠加链表的形式解决。
## 具体实现(代码）

## 运行截图